<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JekyllでLive2Dを実装]]></title>
    <url>%2F2019-07-29-Build-live2d_jpn.html</url>
    <content type="text"><![CDATA[Live2D左下のこの子をどうやって生成したか？この記事で説明する．2次元のキャラクターに命を吹き込むこの偉大なるプロジェクトはLive2D．しかし，プロジェクトはHexo向けで，作者はJekyllでの実装方法を提供していない，どうしよう？！Jekyllで実装私はlazyな人なので，原理を究明してJekyllにコンパイルすることは〜，さすがにしない！ここで「バカ」な方法で解決する．作者がHexoでの実装方法を提供しているから，いっそうHexoでキャラクターをコンパイルして，キャラクターのコードを直接Jekyllに移植すればいいじゃん？JekyllでもHexoでも静的サイトジェネレータでしょう．最終的に，コンパイルの結果が出る．その結果から，キャラクターのコードを探し出すのは今回の要務．Hexoをインストール12345npm install hexo-cli -ghexo init blogcd blognpm installhexo sはい！Hexoのウェブページが出たでしょう．オーケー，問題なし，いったんserverを閉じて，次に行こう．Live2Dをインストール1npm install --save hexo-helper-live2d次に，以下のコードをHexoで生成したブログの_config.xmlに追加する：12345678910111213141516live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false debug: false model: use: shizuku display: position: right width: 150 height: 300 mobile: show: trueひとまず，フレームワークは構築できた．Live2Dのファイルをダンロードする必要がある．git cloneした後，live2d-widget-model-shizuku下のassetsにある内容（assets自体を含まない）を，すべてHexoのblogフォルダ下のlive2d_models/shizukuフォルダ（新規に作って）にコピーする．ファイルのツリーはこうなる：bloglive2d_modelsshizukumoc, mtn, shizuku.model.json...コードを抽出もう一回hexo sを実行して，shizukuちゃんでたでしょう．問題なければ，hexo gでブログをコンパイルする．生成されたpublicの中に，live2dwフォルダがあるはず．その後，public下のindex.htmlを開いて，一番したのこの行を見つける：1&lt;script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"&gt;&lt;/script&gt;&lt;script&gt;L2Dwidget.init(&#123;"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":&#123;"jsonPath":"/live2dw/assets/shizuku.model.json"&#125;,"display":&#123;"position":"right","width":150,"height":300&#125;,"mobile":&#123;"show":true&#125;,"log":false&#125;);&lt;/script&gt;おめでとう！！！shizukuちゃんの本体をつかめた．最後に，live2dwフォルダをJekyllで生成したブログのフォルダに移動，shizukuちゃんの本体を/_includes/_partials/footer.htmlにコピーする．終わり〜〜〜！]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Web</tag>
        <tag>Jekyll</tag>
        <tag>Blog</tag>
        <tag>Japanese</tag>
        <tag>Live2D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pagesでブログを作る]]></title>
    <url>%2F2019-07-26-BuildBlog_jpn.html</url>
    <content type="text"><![CDATA[2019/07/27: 本ブログはもうHexoに引っ越した．Demoがもう見えないけど，このJekyllの記事はまだまだ適用．P.S. Hexoはいいぞ :)このブログはNexT Themeに基づいて作ったもの．Simpleyytに感謝...環境の設定Linuxでの実装過程を示す．（他のOSでの実装方法自分で調べなさい，使うものは同じだから）Ruby1apt install ruby ruby-devRubyGems公式サイトからパッケージをダンロードして，解凍した後，ruby setup.rbでインストール．NodeJS1apt install nodejsBundler1gem install bundler以上で何らかのエーラーが出たら，ライブラリが完備していないかもしれない．その時1apt install build-essential patch ruby-dev zlib1g-devブログの設定まず，NexT Themeをパソコンにgit clone：12git clone https://github.com/Simpleyyt/jekyll-theme-next.gitcd jekyll-theme-next依存をインストール：1bundle installJekyllを実行：1bundle exec jekyll serverこの時，ブラウザでhttp://localhost:4000にアクセスしたら，以下の画面が見えるはず，おめでとう！ブログの作成もう完了~後は自分らしいのものに加工して，GitHubに載せるだけ．Theme設定Schemeを選択SchemeはNexTが提供している特性の一つで，Schemeを通して，NexTは様々な外観テーマを提供している．今，NexTには３種類のSchemeがある：MuseMistPiscesこのブログの外観テーマはMuseで，他の２つのテーマは自分で変えてみて，好きなものを選べればいい．Schemeの変更はフォルダにある_config.ymlをいじればいい，schemeを検索して，対応するセッティングを見える．使用したいScheme前のコメントを消せばOK．123#scheme: Muse#scheme: Mistscheme: Pisces言語の設定NexTは多様な言語をサポートしている．_config.ymlを編輯することで，言語サポートを指定する．例えば，日本語にしたい場合は，以下のように設定：1language: ja今NexTが対応している言語は下表に示す：言語コード設定例Englishenlanguage: en简体中文zh-Hanslanguage: zh-HansFrançaisfr-FRlanguage: fr-FRPortuguêsptlanguage: pt or language: pt-BR繁體中文zh-hk 或者 zh-twlanguage: zh-hkРусский языкrulanguage: ruDeutschdelanguage: de日本語jalanguage: jaIndonesianidlanguage: idKoreankolanguage: koメニューの設定メニューは３つの部分から構成されている，１つ目はメニュー項目（名称とリンク），２つ目は表示するテキスト，３つ目はメニュー項目に対応するアイコン．NexTは Font Awesomeのアイコンを利用しており，600+のアイコンを提供し，ほとんどの使用シーンをカバーできると同時に，アイコンがRetinaスクリンでのぼやける問題も心配する必要がない．_config.ymlを編輯して，メニューを設定しましょう．メニュー項目の設定，フォマットは項目名:リンク.1234567menu: home: / archives: /archives #about: /about #categories: /categories tags: /tags #commonweal: /404.htmlNextにあるデフォルトの項目は以下に示す：Key設定値表示するテキスト（日本語）homehome: /ホームarchivesarchives: /archivesアーカイブcategoriescategories: /categoriesカテゴリtagstags: /tagsタグaboutabout: /aboutAboutメニュー項目のアイコンを設定，フォマットは項目名：アイコン名，アイコン名はここから調べられる．enableの値をfalseにすると，アイコンは消える．12345678menu_icons: enable: true # Icon Mapping. home: home about: user categories: th tags: tags archives: archiveサイドバーの設定デフォルトの場合，サイドバーは文章のページ（見出しがある時）でしか出ない．ポジションは右側．_config.ymlを編輯することで，サイドバーのポジションと出現タイミングを変えられる．ポジション（sidebar.position）left - 左寄りright - 右寄りP.S. ポジションの変更は Pisces Schemeのみで利用可能．12sidebar: position: leftサイドバーの出現タイミング（sidebar.display）post - デフォルト，文章のページ（見出しがある時）で現れるalways - 全てのページで現れるhide - 全てのベージで隠れる（クリックで展開できる）remove - 完全削除12sidebar: display: postP.S サイドバーはuse motion: false（サイトのアニメーションを利用しない）の場合，出現しない．プロフィール画像の設定_config.ymlをオープン，avatarのところを編輯する．画像のリンクはインタネットのリンクでもいいし，サイト内のリンクでもいい：リンク値インタネットのurlhttp://example.com/avatar.pngサイト内のリンク画像を assets/images/ のフォルダに置く， 設定は：avatar: /assets/images/avatar.pngプロフィール画像の設定例：1avatar: http://example.com/avatar.png作者のニックネームの設定_config.ymlにあるauthorをほしいニックネームに設定する．サイトの紹介_config.ymlにあるdescriptionのところに，サイトの紹介を書く．好きな座右の銘を書いても構わない :)サードパーティのサービスNexTに様々なサードパーティのサービス（コメントとか字数の統計とか）が統合されてある．自分の必要に応じて，_config.ymlをいじってください．Live2Dの追加私のブログを見た方々は，この左下の子に気づいたでしょう．この子はlive2dという技術で作られた子で，サイトに追加する過程がちょっと複雑で，他の記事に書く．GitHubにアップロードさあ，どうやって自分のブログをインタネットを通して他の人に見せるの？サーバーに載せればいい．しかし，サーバーはどうやって入手する？自分で作るかクラウドサーバーを買う．どれも時間とお金をかかる面倒くさい作業．幸いなことに，GitHubでGitHub Pagesというサービスを利用することで，誰でも簡単に，而も無料でウェブサイトを作ることができる．GitHubのアカウントを作るリポジトリを新規に作成リポジトリの名前は自分のサイト名.github.ioUpload filesをクリックし，ブログのフォルダをアップロードする自分のサイト名.github.ioをブラウザに入力すれば，ブログは出てくるよ！今，このリンクをアクセスすれば，世界の誰でもあなたのブログを見ることができる！！！]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Web</tag>
        <tag>Jekyll</tag>
        <tag>Blog</tag>
        <tag>Japanese</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫教程二：正则表达式]]></title>
    <url>%2F2018-09-18-CrawlerTutorial2_zh.html</url>
    <content type="text"><![CDATA[了解正则表达式正则表达式，又称规则表达式。（英语：Regular Expression，在代码中常简写为regex、regexp或RE），计算机科学的一个概念。正则表达式通常被用来检索、替换那些符合某个模式(规则)的文本。 (百度百科)正则表达式，说白了就是一种匹配规则，它规定的特定的字符可以匹配特定的东西。就像给事物分类一样，给了一个类名，就能归纳出同一类的东西。只不过正则的语法系统非常复杂，所以精通正则的人基本上可以在网页中大海捞针了。对于初学者来说，没必要硬记它的表达，先过一遍它里面什么就行，日后对照着正则表达式的表，边看边写，慢慢就记住了。Python自带的re模块，就提供了对正则表达式的支持，下面就让我们实践出真理，边做边学。正则表达式的语法正则总结表此表出自CSDN，详细地总结了python支持的正则的语法规则，大家先大概看一遍，后面我们再来慢慢熟悉。Python的re模块re.match(pattern, string, flags=0)match()方法从字符串string的开头开始匹配pattern，匹配成功返回一个Match对象，否则便返回None。至于flags参数之后再说。1234567891011import restring = 'Who is more attractive, Gakki or Satomi?'r = re.match('(\S+) (\S+)',string)print(r)# &lt;_sre.SRE_Match object; span=(0, 6), match='Who is'&gt;print(r.group())# Who isprint(r.group(1))# Whoprint(r.group(2))# is\S匹配非空字符,+表示匹配前一个字符一次或一次以上。match()是从字符串开头开始匹配的，所以第一个非空字符为‘W’，因为有+，所以继续往后匹配，匹配到&#39; &#39;(空字符)的时候，第一个\S+不再匹配了，匹配完空格之后，第二个\S+继续匹配非空字符，直到遇见&#39; &#39;为止，匹配结束，返回Match对象。接下来如何从Match对象中取出匹配结果呢？答案就是group()方法。group()或者group(0)获取整个表达式的匹配结果，group(1)获取表达式中第一个括号的结果，group(2)获得表达式中第二个括号的结果，group(n)就是获取第n个括号的结果了。如果group()方法访问的结果不存在，就会报错IndexError: no such group。re.search(pattern, string, flags=0)search()的用法和match()基本一致，唯一的不同就是search会扫描整个string，从符合pattern的位置开始匹配，而match()只从string的开头开始匹配，如果开头不符合pattern的表达式的话直接返回None。1234567891011121314import restring = 'Who is more attractive, Gakki or Satomi?'r = re.match('(\S+) or (\S+)',string)print(r)# &lt;_sre.SRE_Match object; span=(24, 40), match='Gakki or Satomi?'&gt;print(r.group())# Gakki or Satomi?print(r.group(1))# Gakkiprint(r.group(2))# Satomi?r = re.match('(\S+) or (\S+)',string)print(r)# Nonere.findall(pattern, string, flags=0)前面的match(),search()都只会找到第一个符合pattern的结果，只返回一个结果。而findall()则会找出所有符合pattern的结果，以列表的形式返回。12345import restring = '1,2,3,a,b,c'r = re.findall('\d',string)print(r)# ['1', '2', '3']re.finditer(pattern, string, flags=0)用法与findall()一样，只不过返回的是所有结果(Match对象)的迭代器(generator)。12345678910import restring = '1,2,3,a,b,c'r = re.finditer('\d',string)print(r)# &lt;callable_iterator object at 0x7f586660ad68&gt;for i in r: print(i.group())# 1# 2# 3re.sub(pattern, repl, string, count=0, flags=0)pattern在string中匹配到的结果，以repl来替换，count指定替换的次数,默认为替换全部。12345import restring = 'xxx is so cute!'r = re.sub('\S+','Gakki',string,1)print(r)# Gakki is so cute!re.split(pattern, string, maxsplit=0, flags=0)以匹配到pattern的字符切割字符串，返回列表。maxsplit指定分割次数，默认为分割全部。12345import restring = 'IsiharaxSatomixgaxkawaii!'r = re.split('x',string)print(r)# ['Isihara', 'Satomi', 'ga', 'kawaii!']re.compile(pattern, flags=0)compile()方法用于编译正则表达式，生成一个正则表达式对象，供上述方法调用。当一个pattern被反复利用时，用compile()编译好更方便。然后说一下flags这个参数，前面的方法中也出现了，一般默认不写就行，但是有特殊要求的话，可以指定,具体选项如下：re.S(DOTALL): 匹配任意字符，包括换行符&#39;\n&#39;re.I(IGNORECASE): 忽略大小写re.M(MULTILINE): 多行模式，改变&#39;^&#39;和&#39;$&#39;的行为（参见总结表）re.L(LOCALE): 使预定字符类 \w \W \b \B \s \S 取决于当前区域设定re.U(UNICODE): 使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性re.X(VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。123456789import restring = '''Gakki is always selected into MOST IDEAL GIRLFRIEND RANKING.Satomi is always selected into MOST IDEAL GIRLFRIEND RANKING.'''pattern = re.compile('(\S+) is')r = pattern.findall(string)print(r)# ['Gakki', 'Satomi']除了上面这种调用方法，pattern还可以作为search(),find()...的参数被调用。123456import restring = "In according to unofficial reports, many programmers claims that they are Gakki's husbands."pattern = re.compile('gakki',re.I)r = re.search(pattern,string)print(r.group())# Gakki]]></content>
      <categories>
        <category>Crawler</category>
      </categories>
      <tags>
        <tag>Crawler</tag>
        <tag>Python</tag>
        <tag>Chinese</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Uploaded Some Pics I Took]]></title>
    <url>%2F2018-09-16-UploadedSomePics.html</url>
    <content type="text"><![CDATA[Photo by @S-W-K]]></content>
      <categories>
        <category>Photography</category>
      </categories>
      <tags>
        <tag>Photography</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Crawler Tutorial-1：Basic Structure of a Web Page]]></title>
    <url>%2F2018-09-11-CrawlerTutorial1_en.html</url>
    <content type="text"><![CDATA[In this era of information, everyone should notice the value of information. Wherever there is information, there is also treasure and success! If you can build your own web crawler, it will be easier for you to acquire the huge treasure in the world of Internet.I&#39;m a beginner in the field of web crawler. The purpose of this blog is to record my learning and hope these articles could also help you to learn web crawler. My English is not very good, so it would be appreciated if you could point out my grammar mistakes.The truth of a web pageLet us open a website to have a look. It may look like this:With an elegant format design and some pictures.Actually, when you right-click the web page and choose View Page Source(or something like this), you&#39;ll get these stuff:It looks like some kind of programming language, doesn&#39;t it? We call it HTML(HyperText Markup Language), as you see, it&#39;s a kind of markup language. Except HTML, a web page also contains CSS and JavaScript. They work together to build the web page what we usually see. We won&#39;t go deep into the connections among them.Anyway, when we right-click the web page we can get its source code, and we&#39;ll obtain the data we want from the source code. Basically, every useful content is included in labels like &lt;div&gt;...&lt;/div&gt;，&lt;li&gt;...&lt;/li&gt;, and labels contains attributes like href=&quot;http://www.gakki.cute.com&quot;, class=&quot;Gakki&quot;, or texts like &gt;Gakki is my wife&lt;. These links or texts contained in the labels are usually what we want to download from websites.ToolsFor the sake of acquiring those data from the Internet. We need a handy tool.Life is short, you need Python.Python is really a powerful and easy-learning programming language. Everyone can handle it in a short time. It has simple syntax which is very readable, and there is an active community to support its development. Many geniuses have wrote their libraries for specifical problems. All we need to do is to import their libraries, use the functions they built for us. In a word, you need python and you will not regret to learn it.Python is just like designed to bulid web crawlers, coding in python is a relaxing thing. Lots of libraries are bulit for crawlers, such as BeautifulSoup, lxml, requests, selenium, PyQuery, Urlib, regular expression, etc. I&#39;ve used most of them. As far as I&#39;m concerned, the following three libraries are most helpful:requestslxmlregular expressionrequests is used to send a request to a server and get a webpage source, lxml and regular expression are used to parse the page source. If you choose Chrome as your browser like me, it will be very easy to get the xpath of an element or a lable for lxml to locate.Lastly, requests and lxml are third-party libraries, you need to install them first:12pip install requestspip install lxmlP.S. The series of tutorials is based on python3, and multiprocessing(A tech can help accelerate your crawler which I&#39;ll talk about in another tutorial) is much easier to accomplish in python3 than python2.So, download python3 and start your coding.]]></content>
      <categories>
        <category>Crawler</category>
      </categories>
      <tags>
        <tag>Crawler</tag>
        <tag>Python</tag>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫教程一：网页的基本结构]]></title>
    <url>%2F2018-09-11-CrawlerTutorial1_zh.html</url>
    <content type="text"><![CDATA[网上全是想要的数据，亦或是美图，甚至是教育片，想要收藏进硬盘，中饱私囊。奈何没有适合的手段，那么爬虫就是为你量身定做的利器了。自己也是走了一些弯路，学会了点皮毛。觉得爬虫确实是个好东西，于是就把一些心得写在这里，希望能帮助到有需要的人。网页的真实面目鼠标随手点开一个网页，恩～？！它是这样的：整齐的文字排版，精美的图片，让我们眼花缭乱。实际上鼠标右击，检查源代码会发现，真是情况是这样的：看起来像不像一串代码？这就是书写网页的语言HTML了。当然除了HTML, 一同渲染网页的还有CSS和JavaScript。在这里先不深入了，总之浏览器里右击检查源代码就能褪去网页的外衣，一窥其本貌。我们后期就靠着分析这源码页面来爬取想要的内容。在HTML中基本上实质内容都被&lt;div&gt;...&lt;/div&gt;，&lt;li&gt;...&lt;/li&gt;这样的标签包围着，标签里面又有着href=&quot;http://www.gakki.cute.com&quot;，class=&quot;Gakki&quot;之类的属性，或者包含着&gt;Gakki is my wife&lt;这样的文本。我们要爬取的也基本就是这些隐藏在标签中的链接和文本了。工具有米之炊无巧妇难为，要把资源装进包，趁手利器不可少。人生苦短,我用 Python。作为如今最火的语言之一，不得不说，python确实牛逼。首先语法非常简洁明了，其次社区火爆，各种第三方库的支持，好多实现大神们早已写好，你只要import一下就足够了。网上关于python的教程很多，大家可以自行查阅，学完就知道了，真的非常方便。在爬虫领域，python就像是被设计用来干这差事的，用起来十分顺手。爬虫相关的库有BeautifulSoup，lxml，requests，selenium，PyQuery，Urlib，正则...各种各种。我基本上都多多少少都接触过，用来用去，觉得还是这三件套效率最高:requestslxml正则requests用来模拟浏览器向服务器发出请求获取网页，lxml和正则用来解析网页。如果你用的浏览器是chrome，那么与lxml的xpath语法配合起来就是开挂。哦，忘了说前两个是python的第三方库，需pip安装：12pip install requestspip install lxmlP.S. 整个教程是基于python3的，后面的多进程用python3来写方便很多:)，所以一起用python3吧！]]></content>
      <categories>
        <category>Crawler</category>
      </categories>
      <tags>
        <tag>Crawler</tag>
        <tag>Python</tag>
        <tag>Chinese</tag>
      </tags>
  </entry>
</search>
